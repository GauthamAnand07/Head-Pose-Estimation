import numpy as np # To handle numerical
import cv2 # for computer vision
import mediapipe as mp # to use fase mesh 
import time 

mp_face_mesh = mp.solutions.face_mesh 
# Initializes the Face Mesh model from MediaPipe

face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5,min_tracking_confidence=0.5)
# calls the FaceMesh func from the mo_face_mesh and sends some of the parameters
# min_detection_confidence=0.5 is basically a threshold

mp_drawing = mp.solutions.drawing_utils
# Initializes the drawing functions for drawing landmarks

drawing_spec = mp_drawing.DrawingSpec(color=(128,0,128),thickness=2,circle_radius=1)
# Calls the DrawingSpec func from the mp_drawing and sends some parameters
# color=(128,0,128) initializes that th color of landmark points purple
# thickness=2 initializes the Thickness of the lines connecting the landmarks.
# circle_radius=1 initializes the Radius of the circles representing the landmarks.

cap = cv2.VideoCapture(0)
# Starts the webcam

while cap.isOpened():
    success, image = cap.read()
    # While the camera is open,success indicates if the frame was successfully read and image contains the current frame. 

    start = time.time() # Records the start time to calculate FPS later

    image = cv2.cvtColor(cv2.flip(image,1),cv2.COLOR_BGR2RGB)
    # cv2.color is used to conver the image into RGB from the opencvs BGR format.
    # OpenCV loads images in BGR format by default, but many libraries, like Matplotlib, expect images in RGB format.
    # cv2.flip(image, 1) flips the image horizontally. 1 indicates a horizontal flip, 0 indiactes vertical and -1  both flips

    image.flags.writeable = False 
    # Makes the image read-only to optimize processing speed for MediaPipe and also restricts modifications.

    results = face_mesh.process(image)
    # The image is sent to the process func in the face_mesh model which detects facial landmarks in the image.

    image.flags.writeable = True 
    # Makes the image writable again thus allowing modifications.

    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)
    # The image is converted back to BGR for making it Opencv compatable.

    img_h , img_w, img_c = image.shape 
    # Extracts and stores the image height, width and channell number from the image.shape
    face_2d = []
    face_3d = []
    # Initializes 2 empty lists to store 2d and 3d facial landmark coordinates

    if results.multi_face_landmarks: # if landmarks are detected in results 
        for face_landmarks in results.multi_face_landmarks: # iterates over the landmarks for each detected face.
            for idx, lm in enumerate(face_landmarks.landmark): # iterates over each landmark in the current face's landmarks. The enumerate function provides both the index (idx) and the landmark (lm) itself.
                if idx == 33 or idx == 263 or idx ==1 or idx == 61 or idx == 291 or idx==199: # Checks and verifies if all the indxs represent the desired landmark
                    if idx ==1: # for nose index
                        nose_2d = (lm.x * img_w,lm.y * img_h) 
                        # 2D coordinate is created by scaling the landmark's x and y values (which are between 0 and 1) by the image width (img_w) and height (img_h).
                        nose_3d = (lm.x * img_w,lm.y * img_h,lm.z * 3000)
                        #  3D coordinate is created by including the z value (depth) scaled by a factor (in this case, 3000).

                    x,y = int(lm.x * img_w),int(lm.y * img_h)
                    # For each landmark, its 2D coordinates are calculated by multiplying its x and y values by the image dimensions and converting them to integers.

                    face_2d.append([x,y]) # The coordinates are appended in the face_2d list.
                    face_3d.append(([x,y,lm.z])) # The coordinates along with z are appended in the face_3d list.


            face_2d = np.array(face_2d,dtype=np.float64)
            face_3d = np.array(face_3d,dtype=np.float64)
            # The 2d and 3d lists are converted into numpy arrays


            focal_length = 1 * img_w
            # The focal_length is assigned as the width. *1 for scalability and to make changes 

            cam_matrix = np.array([[focal_length,0,img_h/2],
                                  [0,focal_length,img_w/2],
                                  [0,0,1]])
            # Basically contains the parameters of the camera, which include details like the focal length and the optical center of the camera.
            # SYNTAX. 
            # [ fx   0   cx ]
            # [ 0   fy   cy ]
            # [ 0    0    1 ]
            
            # fx and fy are the focal lengths in x and y axis which determines how much the camera focuses light onto the image sensor.
            # here fx and fy are set to width
            
            # cx and cy are the coordinates of the principal points(the center of the image). 
            # cx is set to img_h / 2, and cy is set to img_w / 2, indicating the optical center of the camera is assumed to be at the center of the image. 
            
            # The third row [0, 0, 1] is standard and helps maintain the mathematical structure required for homogeneous coordinates in computer vision.

            distortion_matrix = np.zeros((4,1),dtype=np.float64)
            # the distortion_matrix is set to all zeros which means no lens distortion is being considered in the calculations.
            # This accounts for any distortion caused by the camera's lens. 

            success,rotation_vec,translation_vec = cv2.solvePnP(face_3d,face_2d,cam_matrix,distortion_matrix)
            # Uses cv2.solvePnP() to estimate the head pose by finding the rotation and translation vectors that map 3D points to 2D projections.
            # These vectors describe the orientation and position of the 3D object (the face) in the camera's coordinate space.  

            #getting rotational of face
            rmat,jac = cv2.Rodrigues(rotation_vec)
            # cv2.Rodrigues() converts the rotation vector into a rotation matrix.

            angles,mtxR,mtxQ,Qx,Qy,Qz = cv2.RQDecomp3x3(rmat)
            # cv2.RQDecomp3x3() decomposes the rotation matrix to get Euler angles.

            x = angles[0] * 360
            y = angles[1] * 360
            z = angles[2] * 360
            # The angles are multiplied by 360 to convert them into degrees.

            if y < -8:
                text="Looking Left"
            elif y > 8:
                text="Looking Right"
            elif x < -10:
                text="Looking Down"
            elif x > 10:
                text="Looking Up"
            else:
                text="Forward"
            # The rotational angles(degrees) are considered 
            # y: Controls left/right orientation.
            # x: Controls up/down orientation.
            # If none of the conditions are met, the head is considered to be looking forward.    

            nose_3d_projection,jacobian = cv2.projectPoints(nose_3d,rotation_vec,translation_vec,cam_matrix,distortion_matrix)
            # cv2.projectPoints() function is used to project a 3D point (here, nose) into a 2D image plane using the rotation and translation vectors obtained earlier
            # The main purpose of the head pose estimation is to determine which direction a person is looking (e.g., up, down, left, right, or forward). Simply having the 3D orientation vectors (rotation and translation vectors) doesn't directly translate to a clear visual indication on the image.
            # By projecting a 3D point (like the nose position) into 2D space, you can draw a line that shows the direction of the head. This visual cue makes it intuitive for the user to understand the head's orientation.

            p1 = (int(nose_2d[0]),int(nose_2d[1]))
            p2 = (int(nose_2d[0] + y*10), int(nose_2d[1] -x *10))
            # p1 represents the starting point of the line that will be drawn on the image. It corresponds to the 2D position of the nose on the image.
            # nose_2d[0] is the x-coordinate of the nose in the 2D image space.
            # nose_2d[1] is the y-coordinate of the nose in the 2D image space.
            
            # p2 is the endpoint of the line that will be drawn to indicate the head's direction.

            cv2.line(image,p1,p2,(255,0,0),3)
            # To display the defined line for the angle

            cv2.putText(image,text,(20,50),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),2)
            cv2.putText(image,"x: " + str(np.round(x,2)),(500,50),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)
            cv2.putText(image,"y: "+ str(np.round(y,2)),(500,100),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)
            cv2.putText(image,"z: "+ str(np.round(z, 2)), (500, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            # To Display the text on the screen


        end = time.time()
        totalTime = end-start

        #fps = 1/totalTime
        #print("FPS: ",fps)

        #cv2.putText(image,f'FPS: {int(fps)}',(20,450),cv2.FONT_HERSHEY_SIMPLEX,1.5,(0,255,0),2)

        mp_drawing.draw_landmarks(image=image,
                                  landmark_list=face_landmarks,
                                  connections=mp_face_mesh.FACEMESH_CONTOURS,
                                  landmark_drawing_spec=drawing_spec,
                                  connection_drawing_spec=drawing_spec)
    cv2.imshow('Head Pose Detection',image)
    if cv2.waitKey(5) & 0xFF ==27:
        break
cap.release()
